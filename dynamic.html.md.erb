---
title: S3 Dynamic Storage
owner: Services
---

<strong><%= modified_date %></strong>

## <a id='overview'></a> Overview

Dynamic Storage, the Cloud Storage of Swisscom, is a pay-per-use object storage optimized for storing large data sets (e.g. for backups, archives, big-data etc.) for your applications. From these, you can access your stored data through a RESTful API.

Swisscom Dynamic Storage supports two APIs:

* [Atmos API](https://www.emc.com/techpubs/vipr/ds_atmos_supported_features-2.htm)
* [Amazon S3 API](https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html) (Currently only [V2 signing](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html) is supported. There is no support for V4 signing yet.)

Both APIs are RESTful APIs but have a slightly different syntax.

The Atmos API has a lot of very high sophisticated functions, like:

* Different policies on each object
* Invitations and invitation limitations
* And many more...

The Amazon S3 API is:

* Very well known in the market
* State of the art, de facto standard
* Providing a big community
* Has Hundreds of scripts and tools available

<p class='note'>
  <strong>Important</strong>: Swisscom does not support access to the same data with both APIs. There are different accounts and access points. There is no migration path from the Atmos API to the S3 API or vice versa.
</p>

## <a id='integrating-your-service'></a> Integrating the Service With Your App

After the [creation](../devguide/services/managing-services.html#create) of the service and the [binding](../devguide/services/application-binding.html#bind) of the service to the application, the environment variable [VCAP_SERVICES](../devguide/deploy-apps/environment-variable.html#VCAP-SERVICES) is created. Information about the credentials are stored in this variable as shown here:

```json
{
  "dynstrg": [
    {
      "credentials": {
        "accessHost": "ds31s3.swisscom.com",
        "accessKey": "SubtenantID/AccountID",
        "sharedSecret": "28-Characters"
      },
      "label": "dynstrg",
      "name": "testatmos",
      "plan": "usage",
      "tags": []
    }
  ]
}
```

## <a id='sample-application'></a> Sample Application

Swisscom: [Dynamic Storage How-To](https://github.com/swisscom/dynstrg-howto)

## <a id='How'></a> How to access Dynamic Storage with the Amazon S3 API

|                           | Amazon S3                             | Swisscom S3 Compatible Storage         |
| ------------------------- | ------------------------------------- | -------------------------------------- |
| Access                    | Access Key ID                         | Access Key ID                          |
| Access Key                | Secret Access Key                     | Secret Access Key                      |
| Bucket Path Style         | `http[s]://s3.amazonaws.com/<bucket>` | `https://ds31s3.swisscom.com/<bucket>` |
| Bucket Virtual Host Style | `http[s]://<bucket>.s3.amazonaws.com` | `https://<bucket>.ds31s3.swisscom.com` |

<p class='note'>
  <strong>Impotant</strong>: for security reasons, Swisscom does only support HTTPS (443) and no HTTP (80)!
</p>

## <a id='Using'></a> Using Amazon S3 compatible software on Dynamic Storage

Most of the software that has been developed to use the Amazon S3 API will work with Dynamic Storage as long as they support [V2 signing](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). The unsupported operations are rarely used and listed below.

The challenge is often on the configuration side: Sometimes the Amazon URL and ports have been hard coded, sometimes the software is using path style and sometimes virtual style.

## <a id='authentication'></a> Authentication

<p class='note'>
  <strong>From the <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html">AWS S3 docs</a>:</strong>
  The Amazon S3 REST API uses a custom HTTP scheme based on a keyed-HMAC (Hash Message Authentication Code) for authentication. To authenticate a request, you first concatenate selected elements of the request to form a string. You then use your AWS secret access key to calculate the HMAC of that string. Informally, we call this process "signing the request," and we call the output of the HMAC algorithm the signature, because it simulates the security properties of a real signature. Finally, you add this signature as a parameter of the request by using the syntax described in this section. When the system receives an authenticated request, it fetches the AWS secret access key that you claim to have and uses it in the same way to compute a signature for the message it received. It then compares the signature it calculated against the signature presented by the requester. If the two signatures match, the system concludes that the requester must have access to the AWS secret access key and therefore acts with the authority of the principal to whom the key was issued. If the two signatures do not match, the request is dropped and the system responds with an error message.
</p>

When using Atmos, the <strong>Access Key ID corresponds to Subtenant ID and Account ID</strong> and the <strong>Secret Access Key corresponds to the Shared Secret Access Key</strong>.

In the newer versions of Amazon SDK the signing method have changed. In order to make sure that you can use the SDK correctly, you need to overwrite the signing method to use V2 signing. This would look like the following example in Java code:

```java
ClientConfiguration opts = new ClientConfiguration();
opts.setSignerOverride("S3SignerType");  // NOT "AWS3SignerType"

AmazonS3 s3 = new AmazonS3Client(new BasicAWSCredentials("UID", "sharedSecret"), opts);
```

## <a id='getting'></a> Getting better write performance

To get the best possible performance (excerpt from the <a href="https://community.emc.com/docs/DOC-3481" taget="_blank">Atmos Programmer’s Guide</a>):

* Write as many objects in parallel as possible.
* Use only one request per object for creates and updates. Concurrent access to the same object (on writes) may lead to locking or serialization overhead.
* Use the `expect: 100-continue` request header when creating or updating namespace objects. By using the expect header, the requesting application can avoid sending data when it would fail. Here’s how it works:

  1. The application requesting the create/update sends the `expect: 100-continue` header in the request, along with the other normal request headers. But they do not send any data.
  1. The system receives the request, validates the headers, and checks to see if an object with that name exists.
  1. If everything is ok, the system sends back the 100-continue response.
  1. If there is an error, (for example the signature is invalid or the object name already exists) the system sends back the normal error response, and the application never sends the request body.
  1. The application will only send data if it gets the 100-continue header back.

## <a id='bucket'></a> Amazon S3 bucket configuration

S3 bucket names are unique through the system. So in case, you get an error while creating a bucket, then please chose a different name.

S3 buckets can be configured in either flat mode or one-to-one mode. Choosing the mode to use depends on whether the keys are hierarchical or not. One-to-one mode is the default. Set the bucket’s mode immediately after creating the bucket. If you change the mode after objects have been ingested, those objects become inaccessible.
Currently, bucket type change cannot be done by the customer.
If you need to change the bucket type, please open an incident and ask for type change.

### <a id='one-to-one'></a> One-to-one (default)

One-to-one mode supports applications that treat S3 keys like a traditional directory structure with prefixes separated by slashes. In one-to-one mode, prefix searching performs well as long as you use the standard delimiter (“/”).
**One-to-one mode’s limitation is that you are limited to 64,000 objects per directory (prefix).**
A key in one-to-one mode (or hierarchical mode) looks like this:

```URL
/bucket/articles/2013-09/document234/images/header.jpg
```

### <a id='flat'></a> Flat mode

Flat mode provides the best performance for applications that store millions of keys and the keys are not hierarchical. An example of a flat key is a UUID. Applications that store objects without any hierarchy generally put all of their keys in the root of the bucket without a prefix such as a directory. **Flat mode’s limitation is that listing buckets using the prefix option is not supported.** A key using flat mode looks like this:

```URL
/<bucket name>/a1e8eb5c-dc2a-4a02-a71b-0fab8bb28c3b
```

If you need to change the bucket type, please open an incident and ask for mode change.

## <a id='s3-api'></a> The Amazon S3 API operations

### <a id='supported'></a> The following operations are supported

#### Bucket operations

* DELETE Bucket
* GET Bucket
* GET Bucket ACL
* GET Bucket Location
* GET Bucket Versioning
* GET Service
* List Multipart Uploads
* PUT Bucket
* PUT Bucket ACL

#### Object operations

* DELETE Object
* DELETE Multiple Objects
* GET Object
* GET Object ACL
* HEAD Object
* POST Object
* PUT Object
* PUT Object ACL
* Multipart Upload – Initiate, Upload, Complete, Abort, List parts

### <a id='unsupported'></a> The following operations are unsupported: (these functions are used very rarely)

#### Bucket operations

* DELETE Bucket CORS
* DELETE Bucket lifecycle
* DELETE Bucket policy
* DELETE Bucket tagging
* DELETE Bucket website
* GET Bucket CORS
* GET Bucket lifecycle
* GET Bucket tagging
* GET Bucket policy
* GET Bucket notification
* GET Bucket logging
* GET Bucket requestPayment
* GET Bucket website
* PUT Bucket CORS
* PUT Bucket lifecycle
* PUT Bucket logging
* PUT Bucket notification
* PUT Bucket policy
* PUT Bucket requestPayment
* PUT Bucket tagging
* PUT Bucket versioning
* PUT Bucket website

#### Object operations

* GET Object torrent
* GET Object versions
* POST Object restore

### <a id='unsupported'></a> The software doesn't use the Amazon S3 API directly, but S3QL

S3QL is a file system that stores all its data online using storage services like <a href="http://code.google.com/apis/storage/">Google Storage</a>, <a href="http://aws.amazon.com/s3AmazonS3">Amazon S3</a> or <a href="http://openstack.org/projects/storage/">OpenStack</a>. S3QL effectively provides a hard disk of dynamic, infinite capacity that can be accessed from any computer with internet access running Linux, FreeBSD or OS-X.
S3QL is a standard conforming, full featured UNIX file system that is conceptually indistinguishable from any local file system. Furthermore, S3QL has additional features like compression, encryption, data de-duplication, immutable trees and snapshotting which make it especially suitable for online backup and archival.
The S3 compatible backend allows S3QL to access any storage service that uses the same protocol as Amazon S3. The storage URL has the form:

```URL
s3c://<hostname>:<port>/<bucketname>/<prefix>
```

Using a S3 compatible url, Dynamic Storage can easily be used as a backend.

### <a id='cyberduck'></a> GUI Client Cyberduck

Dynamic Storage, the Cloud Storage of Swisscom, can be tested with [Cyberduck Client](https://cyberduck.io).
On StackOverflow there is a [how-to for Atmos](http://stackoverflow.com/a/41574931)
